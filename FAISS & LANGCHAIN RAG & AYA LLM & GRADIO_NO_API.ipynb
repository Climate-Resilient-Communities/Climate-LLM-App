{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47dc5aeb-9a1b-4312-99f6-45c68801bb6d",
   "metadata": {},
   "source": [
    "# FAISS & LANGCHAIN RAG & AYA LLM & GRADIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256c884-95a8-4dd6-ba0a-1c08081b2571",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Aya 101 is a state-of-the-art, open-source, massively multilingual large language model (LLM) developed by Cohere for AI. It has the remarkable capability of operating in 101 different languages, including over 50 that are considered underserved by most advanced AI models.\n",
    "\n",
    "### In this notebook, we will go through a step-by-step process of deploying and using the Aya model. We will also build a FAISS powered RAG pipeline using Aya and showcase how enterprises can use this for building AI applications.\n",
    "\n",
    "### The Aya 101 Model by Cohere for AI\n",
    "Aya 101 Model by Cohere for AI project is part of an open science endeavor and is a collaborative effort involving contributions from people across the globe.\n",
    "\n",
    "Aya's goal is to address the imbalance in language representation within AI by developing a model that understands and generates multiple languages, not just the ones that are predominantly represented online.\n",
    "\n",
    "### Key Facts about Aya\n",
    "#### - Massively Multilingual: The model supports 101 languages. It also includes over 50 languages rarely seen in AI models.\n",
    "#### - Open Source: The model, training process, and datasets are all open source.\n",
    "#### - Groundbreaking Dataset: Aya comes with the largest multilingual instruction dataset released till date, comprising 513 million data points across 114 languages.\n",
    "\n",
    "Source: Cohere for : https://www.e2enetworks.com/blog/steps-to-build-rag-pipeline-with-cohere-for-ais-aya-llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c437af0-e71b-408c-bb10-98534e98e078",
   "metadata": {},
   "source": [
    "### Understanding RAG Pipeline\n",
    "The Retrieval-Augmented Generation (RAG) pipeline has become a powerful tool in the field of LLMs. At its core, the RAG pipeline combines two crucial steps:\n",
    "\n",
    "1. Retrieval step: Retrieving relevant stored information using Vector Search or Knowledge Graph or simple search.\n",
    "2. Generation step: Generating coherent text using a combination of contextual knowledge and natural language generation capabilities of LLMs.\n",
    "\n",
    "This combination allows the system to pull in essential details from a database and then use them to construct detailed and informative responses to user queries.\n",
    "\n",
    "### FAISS As Vector Store\n",
    "FAISS, which stands for Facebook AI Similarity Search, is a library developed by Facebook AI that enables efficient similarity search. It provides algorithms to quickly search and cluster embedding vectors, making it suitable for tasks such as semantic search and similarity matching.\n",
    "\n",
    "FAISS can handle large databases efficiently and is designed to work with high-dimensional vectors, allowing for fast and memory-efficient similarity search.\n",
    "\n",
    "In this notebook, we will use FAISS as our Vector Store, which will provide context to the Aya LLM. We will also use LangChain for building the pipeline.\n",
    "\n",
    "### Step-by-Step Guide to Building a RAG Pipeline with Aya\n",
    "\n",
    "### Choosing a GPU node\n",
    "For non API V100 GPU node was used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fb3bf-500b-403a-ac4d-bc7f4eaed3cd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84483f-0bee-409b-91f1-330c66a1b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers langchain faiss-cpu gradio PyPDF2 sentence-transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe2f5a-5dbc-4bce-9c3e-b5009c653248",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e46fa-115e-42f2-8646-e5f0741e2328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc67a1a-d043-4315-a5da-538d85d2aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the quantization config.\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab3587-d608-455d-9969-aaacc27341a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and the tokenizer.\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"CohereForAI/aya-101\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "aya_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, quantization_config=bnb_config,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da820d23-9068-4c81-9780-eaf0f56fcd1f",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90373b52-c35f-4c26-826c-c501eeedbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query pipeline.\n",
    "\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=aya_model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        max_length = 512,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f55420-3dd9-468a-a8e5-b3714c90d370",
   "metadata": {},
   "source": [
    " ### Setting Up a RAG Pipeline wit Langchain andh Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b8189f-2fda-4dcc-af40-c742b93449cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the class, which takes a list of dictionaries representing the document sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f327f-f259-414e-982e-60c8c2f9499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules.\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa668656-d167-4775-be15-8b934948701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text splitter to break down the uploaded documents into smaller chunks.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "chunk_size=1000,\n",
    "chunk_overlap=20,\n",
    "length_function=len,\n",
    "is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3647d-6a06-4255-96cd-61ea9f9040be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an embedding model to vectorize the text in the document.\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843fbc3-b669-4b55-977c-1abab41ede7f",
   "metadata": {},
   "source": [
    "### Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4828a8f-445f-4625-a588-d9ce6c26aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a question-answering chain from the uploaded documents.\n",
    "import gradio as gr\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "def create_retrieval_chain(files):\n",
    "    docs = []\n",
    "\n",
    "    for file_path in files:\n",
    "        if file_path.lower().endswith('.pdf'):  # Check if the file is a PDF\n",
    "            loader_temp = PyPDFLoader(file_path)\n",
    "            docs_temp = loader_temp.load_and_split(text_splitter=text_splitter)\n",
    "            docs += docs_temp\n",
    "        else:\n",
    "            return (f\"Please upload PDF files only\")\n",
    "\n",
    "    for doc in docs:\n",
    "        doc.page_content = doc.page_content.replace('\\n', ' ')\n",
    "\n",
    "    vectordb = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "    retriever = vectordb.as_retriever()\n",
    "\n",
    "    global qa\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "    return f\"Process PDF files. They can be queried now\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0541e-9ae3-47cd-8151-142f439b07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another function to answer the queries based on context retrieved from the documents.\n",
    "def process_query(query):\n",
    "    response = qa.invoke(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87776673-9373-4fbe-99c7-e5c3c3744ae9",
   "metadata": {},
   "source": [
    "### Launch a Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50713193-3704-4407-8937-a2db701891f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Gradio interface\n",
    "iface_save_pdf = gr.Interface(fn=create_retrieval_chain,\n",
    "                     inputs=gr.Files(label=\"Upload Files\", type='filepath'),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"PDF Uploader\",\n",
    "                     description=\"Upload multiple files. Only PDF files will be saved to disk.\")\n",
    "\n",
    "iface_process_query = gr.Interface(fn=process_query,\n",
    "                                   inputs=gr.Textbox(label=\"Enter your query\"),\n",
    "                                   outputs=\"text\",\n",
    "                                   title=\"Query Processor\",\n",
    "                                   description=\"Enter queries to get responses.\")\n",
    "\n",
    "iface_combined = gr.TabbedInterface([iface_save_pdf, iface_process_query], [\"PDF Upload\", \"Query Processor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d45fa-4b44-47cb-85cf-ab005e14d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the combined interface\n",
    "if __name__ == \"__main__\":\n",
    "    iface_combined.launch(server_name='0.0.0.0', server_port=7865, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "app"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
