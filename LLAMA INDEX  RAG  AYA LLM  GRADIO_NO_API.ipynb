{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47dc5aeb-9a1b-4312-99f6-45c68801bb6d",
   "metadata": {},
   "source": [
    "# LLAMAINDEX RAG & AYA LLM & GRADIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256c884-95a8-4dd6-ba0a-1c08081b2571",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Aya 101 is a state-of-the-art, open-source, massively multilingual large language model (LLM) developed by Cohere for AI. It has the remarkable capability of operating in 101 different languages, including over 50 that are considered underserved by most advanced AI models.\n",
    "\n",
    "### In this notebook, we will go through a step-by-step process of deploying and using the Aya model. We will also build a FAISS powered RAG pipeline using Aya and showcase how enterprises can use this for building AI applications.\n",
    "\n",
    "### The Aya 101 Model by Cohere for AI\n",
    "Aya 101 Model by Cohere for AI project is part of an open science endeavor and is a collaborative effort involving contributions from people across the globe.\n",
    "\n",
    "Aya's goal is to address the imbalance in language representation within AI by developing a model that understands and generates multiple languages, not just the ones that are predominantly represented online.\n",
    "\n",
    "### Key Facts about Aya\n",
    "#### - Massively Multilingual: The model supports 101 languages. It also includes over 50 languages rarely seen in AI models.\n",
    "#### - Open Source: The model, training process, and datasets are all open source.\n",
    "#### - Groundbreaking Dataset: Aya comes with the largest multilingual instruction dataset released till date, comprising 513 million data points across 114 languages.\n",
    "\n",
    "Source: Cohere for : https://netraneupane.medium.com/retrieval-augmented-generation-rag-using-llamaindex-and-mistral-7b-228f93ba670f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c437af0-e71b-408c-bb10-98534e98e078",
   "metadata": {},
   "source": [
    "### Understanding RAG Pipeline\n",
    "The Retrieval-Augmented Generation (RAG) pipeline has become a powerful tool in the field of LLMs. At its core, the RAG pipeline combines two crucial steps:\n",
    "\n",
    "1. Retrieval step: Retrieving relevant stored information using Vector Search or Knowledge Graph or simple search.\n",
    "2. Generation step: Generating coherent text using a combination of contextual knowledge and natural language generation capabilities of LLMs.\n",
    "\n",
    "This combination allows the system to pull in essential details from a database and then use them to construct detailed and informative responses to user queries.\n",
    "\n",
    "### LLamaindex As Vector Store\n",
    "lamaIndex is a library that provides a set of tools for building index structures over unstructured or semi-structured data, such as text documents. It leverages various techniques, including vector stores, to efficiently store and retrieve relevant information based on similarity search.\r\n",
    "\r\n",
    "One of the key components of LlamaIndex is its vector store functionality. Vector stores are designed to store high-dimensional vectors, such as embeddings generated from text data, and enable fast similarity search operations. LlamaIndex supports multiple vector store backends, including FAISS (Facebook AI Similarity Search), which is a popular library for efficient similarity search.In this notebook, we will utilize LlamaIndex's vector store capabilities, specifically with the FAISS backend, to store and retrieve relevant context for the Aya language model. LlamaIndex will handle the process of creating an index over the input documents, generating embeddings for each document or passage, and storing them in the vector store.\r\n",
    "\r\n",
    "When a query is made to the Aya language model, LlamaIndex will use the vector store to find the most similar passages or documents based on the query's embedding. This allows us to retrieve the most relevant context for the given query, which can then be used to augment the language model's understanding and generate more accurate and contextually relevant responses.e.\n",
    "\n",
    "### Step-by-Step Guide to Building a RAG Pipeline withused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fb3bf-500b-403a-ac4d-bc7f4eaed3cd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84483f-0bee-409b-91f1-330c66a1b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "!pip install -q pypdf\n",
    "!pip install -q torch\n",
    "!pip install -q transformers\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q llama-index\n",
    "!pip install -q gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe2f5a-5dbc-4bce-9c3e-b5009c653248",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e46fa-115e-42f2-8646-e5f0741e2328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350005e1-3636-4d43-9287-6fa84d6f361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large language model (Aya)\n",
    "from llama_index import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da820d23-9068-4c81-9780-eaf0f56fcd1f",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc67a1a-d043-4315-a5da-538d85d2aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCPP(\n",
    "    model_url='https://huggingface.co/CohereForAI/aya-101-v1-GGUL/resolve/main/aya-101-v1.Q4_K_M.ggul',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    context_window=4096,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f55420-3dd9-468a-a8e5-b3714c90d370",
   "metadata": {},
   "source": [
    " ### Setting Up a RAG Pipeline with llamaindex and Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f327f-f259-414e-982e-60c8c2f9499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document\n",
    "from llama_index import SimpleDirectoryReader, Document\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"./documents/survey_on_llms.pdf\"]).load_data()\n",
    "documents = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa668656-d167-4775-be15-8b934948701b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the index\n",
    "import os\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index import VectorStoreIndex, ServiceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843fbc3-b669-4b55-977c-1abab41ede7f",
   "metadata": {},
   "source": [
    "### Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4828a8f-445f-4625-a588-d9ce6c26aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index\n",
    "import os\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "\n",
    "def get_build_index(documents, llm, embed_model=\"local:sentence-transformers/all-mpnet-base-v2\", sentence_window_size=3, save_dir=\"./vector_store/index\"):\n",
    "    node_parser = SentenceWindowNodeParser(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\"\n",
    "    )\n",
    "\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            [documents], service_context=sentence_context\n",
    "        )\n",
    "        index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return index\n",
    "\n",
    "vector_index = get_build_index(documents=documents, llm=llm, embed_model=\"local:sentence-transformers/all-mpnet-base-v2\", sentence_window_size=3, save_dir=\"./vector_store/index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0541e-9ae3-47cd-8151-142f439b07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query engine\n",
    "from llama_index import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "\n",
    "def get_query_engine(sentence_index, similarity_top_k=6, rerank_top_n=2):\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "    engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return engine\n",
    "\n",
    "query_engine = get_query_engine(sentence_index=vector_index, similarity_top_k=6, rerank_top_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87776673-9373-4fbe-99c7-e5c3c3744ae9",
   "metadata": {},
   "source": [
    "### Launch a Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50713193-3704-4407-8937-a2db701891f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "import gradio as gr\n",
    "\n",
    "def answer_query(query):\n",
    "    response = query_engine.query(query)\n",
    "    return str(response)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=answer_query,\n",
    "    inputs=gr.inputs.Textbox(lines=7, label=\"Enter your query\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"LLM Document Query\",\n",
    "    description=\"Ask a question about the document and get an answer from the LLM.\",\n",
    ")\n",
    "#iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d45fa-4b44-47cb-85cf-ab005e14d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch \n",
    "if __name__ == \"__main__\":\n",
    "iface.launch(server_name='0.0.0.0', server_port=7865, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "app"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
